import streamlit as st
import os
from langchain_groq import ChatGroq
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# --- CONFIGURA√á√ÉO DA P√ÅGINA ---
st.set_page_config(page_title="Civilis SaaS - Mente Jur√≠dica", page_icon="‚öñÔ∏è", layout="wide")

# --- CSS PARA ESTILO PROFISSIONAL ---
st.markdown("""
<style>
    .stChatInput {border-radius: 15px;}
    div[data-testid="stToolbar"] {visibility: hidden;}
    .reportview-container .main .block-container {padding-top: 2rem;}
</style>
""", unsafe_allow_html=True)

# --- CABE√áALHO ---
col1, col2 = st.columns([1, 5])
with col1:
    st.image("https://img.icons8.com/ios-filled/100/4a90e2/law.png", width=80) # √çcone gen√©rico
with col2:
    st.title("Civilis SaaS - Doutrina Secreta")
    st.markdown("**IA Jur√≠dica Baseada em Evid√™ncias e Doutrina**")

st.divider()

# --- CONFIGURA√á√ÉO DE SEGREDOS ---
# Tenta pegar a chave do st.secrets (produ√ß√£o) ou do ambiente local
api_key = st.secrets.get("GROQ_API_KEY") or os.environ.get("GROQ_API_KEY")

if not api_key:
    st.error("‚ö†Ô∏è ERRO CR√çTICO: Chave da API GROQ n√£o encontrada. Configure os 'Secrets' no Streamlit Cloud.")
    st.stop()

# --- MOTOR DE INTELIG√äNCIA (RAG) ---
@st.cache_resource(show_spinner=False)
def carregar_e_processar_pdfs():
    """
    L√™ todos os PDFs do reposit√≥rio, quebra em chunks inteligentes
    e cria o √≠ndice vetorial para busca sem√¢ntica.
    """
    # 1. Carregar PDFs da raiz e subpastas
    pdf_loaders = [
        DirectoryLoader('.', glob="**/*.pdf", loader_cls=PyPDFLoader, show_progress=True)
    ]
    
    docs = []
    for loader in pdf_loaders:
        try:
            docs.extend(loader.load())
        except Exception as e:
            pass # Ignora erros de leitura em arquivos espec√≠ficos

    if not docs:
        return None

    # 2. Dividir em peda√ßos (Chunks) para n√£o estourar a mem√≥ria da IA
    # Chunk de 1000 caracteres com 200 de sobreposi√ß√£o garante contexto cont√≠nuo
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)

    # 3. Criar Embeddings (Transformar texto em n√∫meros)
    # Usamos um modelo leve e gratuito da HuggingFace para rodar r√°pido na nuvem
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    # 4. Criar Banco Vetorial (FAISS)
    vectorstore = FAISS.from_documents(splits, embeddings)
    return vectorstore

# --- INICIALIZA√á√ÉO DO SISTEMA ---
if "vectorstore" not in st.session_state:
    with st.spinner("üîÑ Indexando Doutrina Secreta e Legisla√ß√£o (Isso acontece apenas uma vez)..."):
        vs = carregar_e_processar_pdfs()
        if vs:
            st.session_state.vectorstore = vs
            st.success(f"üìö Base de Conhecimento carregada com sucesso!")
        else:
            st.warning("Nenhum PDF encontrado no reposit√≥rio. O sistema funcionar√° sem contexto espec√≠fico.")
            st.session_state.vectorstore = None

# --- CHATBOT ---

# Modelo LLM (Groq - Llama 3 para velocidade e racioc√≠nio)
llm = ChatGroq(temperature=0.3, model_name="llama3-70b-8192", groq_api_key=api_key)

# Prompt do Sistema (A "Personalidade")
template = """
Voc√™ √© o Assistente Jur√≠dico do Civilis SaaS. Sua mente √© baseada estritamente nos documentos fornecidos.
Use os seguintes peda√ßos de contexto recuperados para responder √† pergunta.
Se voc√™ n√£o souber a resposta baseada no contexto, diga que n√£o consta na doutrina anexada.
Seja t√©cnico, preciso e cite os conceitos jur√≠dicos corretamente.

Contexto:
{context}

Pergunta:
{question}

Resposta Profissional:
"""

QA_CHAIN_PROMPT = PromptTemplate.from_template(template)

if "messages" not in st.session_state:
    st.session_state.messages = []

# Exibir hist√≥rico
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Capturar input do usu√°rio
if prompt := st.chat_input("Pergunte √† Doutrina (Ex: O que diz o C√≥digo Civil sobre usucapi√£o?)"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        
        # L√≥gica de Resposta
        if st.session_state.vectorstore:
            # Modo RAG (Com Consulta aos Livros)
            qa_chain = RetrievalQA.from_chain_type(
                llm,
                retriever=st.session_state.vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 4}),
                chain_type_kwargs={"prompt": QA_CHAIN_PROMPT},
                return_source_documents=True
            )
            
            with st.spinner("Consultando jurisprud√™ncia..."):
                result = qa_chain.invoke({"query": prompt})
                response = result["result"]
                sources = result["source_documents"]
                
                # Formatar resposta + Fontes
                full_response = response + "\n\n---\n**Fontes Consultadas:**\n"
                unique_sources = set()
                for doc in sources:
                    # Tenta pegar o nome do arquivo limpo
                    source_name = os.path.basename(doc.metadata['source'])
                    page = doc.metadata.get('page', 'N/A')
                    unique_sources.add(f"- *{source_name}* (P√°g. {page})")
                
                full_response += "\n".join(unique_sources)
                
                message_placeholder.markdown(full_response)
        else:
            # Modo Fallback (Sem PDFs)
            response = llm.invoke(prompt).content
            message_placeholder.markdown(response)

    st.session_state.messages.append({"role": "assistant", "content": full_response if st.session_state.vectorstore else response})
